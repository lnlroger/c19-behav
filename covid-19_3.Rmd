---
title: "Covid-19: Scoping the field"
author: "Orestis, Lio"
date: "`r format(Sys.time(), '%d %B, %Y')`"
output: 
  bookdown::html_document2: 
    toc : true
    toc_float : true
editor_options: 
  chunk_output_type: console

---

```{r echo = FALSE, message=FALSE, warning=FALSE}
knitr::opts_chunk$set(echo = FALSE, message=FALSE, warning=FALSE, fig.height = 6, fig.width = 12)

library(plyr)
library(tidyverse)
library(stringr)
library(lmerTest)
library(stargazer)
library(mfx)
library(knitr)
library(kSamples)
library(FSA)
library(kableExtra)
library(xtable)
library(miceadds)
library(jtools)
library(plm)
library(clubSandwich)
library(ggrepel)
library(sjPlot)
library(sjmisc)
library(sjlabelled)
library(naniar)
library(ggpubr)
library(lubridate)

options("jtools-digits" = 4,scipen=999) # set displayed decimal places to 4


rm(list = ls())

source("ImportLong.r")

df2<-df%>%
  filter(!is.na(StringencyIndex),!is.na(Movement))
  
  


#Break up w1 by country, then fit specified model to each piec and return a list
models<-dlply(df2,'Country',function(df)
  lm(Movement~StringencyIndex,data=df))

#apply coef to each model and return a data frame
q<-ldply(models,coef)
names(q)<-c("Country","Intercept","Compliance")


#breaks<-df%>%
 # filter(Popular==1)%>%
  #dplyr::select(Country)

df_avg<-df%>%
 filter(!is.na(Movement),!is.na(StringencyIndex))%>%
  group_by(Country)%>%
  summarise_at(vars(-c(total_deaths,total_cases)),mean,na.rn=TRUE)%>%
  ungroup()

df_other<-df%>%
   filter(!is.na(Movement),!is.na(StringencyIndex))%>%
  group_by(Country)%>%
  summarise(total_deaths=last(total_deaths),total_cases=last(total_cases),LogDeath=(log(last(total_deaths)+1)),LogCases=log(last(total_cases)+1),MaxSI=max(StringencyIndex,na.rm=TRUE))%>%
  ungroup()

df_short<-merge(df_avg,df_other,by="Country",all=T)  
df_short<-merge(df_short,q,by="Country",all=T)

#Focusing on the first wave only
w1<-df%>%
  filter(!is.na(StringencyIndex),!is.na(Movement))%>%
  group_by(Country)%>%
  mutate(day=row_number())%>%  ###Putting a counter for each row of each group
  add_tally()%>% ## creating a variable "n" that counts how many rows each group has.
  arrange(Date)%>% 
  mutate(wave1=if_else(StringencyIndex>=dplyr::lag(StringencyIndex),1,0))%>%
  mutate(fir=match(0,wave1)-1)%>% ### first occurence of a 0-value - 1 to discard the 0. 
  mutate(fir=tidyr::replace_na(fir,n()))%>% ### countries that never had a 0 appear as NA. We replace NAs with total rows of 1's
  mutate(wave=ifelse(day<=fir,"first","second"))%>%
  filter(wave=="first")%>%
  ungroup()%>%
  arrange(Country,Date)



#Break up w1 by country, then fit specified model to each piec and return a list
models2<-dlply(w1,'Country',function(df)
  lm(Movement~StringencyIndex,data=df))

#apply coef to each model and return a data frame
wq<-ldply(models2,coef)
names(wq)<-c("Country","Intercept","Compliance")
#Print summary of each model
#l_ply(models, summary, .print = TRUE)





w1_avg<-w1%>%
  filter(!is.na(Movement),!is.na(StringencyIndex))%>%
  group_by(Country)%>%
  dplyr::summarise_at(vars(-c(total_deaths,total_cases)),mean,na.rn=TRUE)%>%
  ungroup()

w1_other<-w1%>%
    filter(!is.na(Movement),!is.na(StringencyIndex))%>%
  group_by(Country)%>%
  dplyr::summarise(total_deaths=last(total_deaths),
            total_cases=last(total_cases),
            LogDeath=log(last(total_deaths)+1),
            LogCases=log(last(total_cases)+1),
            MaxSI=max(StringencyIndex))

w1_short<-merge(w1_avg,w1_other,by="Country",all=T)  

w1_short<-merge(w1_short,wq,by="Country",all=T)


vars_basic<-c("log_GDP_pc","Unemployment","Pop_Density_sq_mi","Age_65up","Death_disease")
vars_basic_GDP<-c("Unemployment","Pop_Density_sq_mi","Age_65up","Death_disease")
vars_briq<-c("risktaking","patience","negrecip","trust","posrecip","altruism")
vars_hof<-c("COL","PDI","MAS","UAI")

```

# Data description
I compiled the data set by collecting country-level variables from a host of sources. 

* google mobility reports: displays the change in visits to places like grocery stores and parks as of 29/03/2020 (the report was released on the 2nd of April I think but it covers up to 3 days before). 
  * The beginning of the observation period (compared to the benchmark) is Feb. 23. This seems to be consistent across all countries.
  * The baseline is the median value, for the corresponding day of the week, during the 5- week period Jan 3â€“Feb 6, 2020.
  * There is a new update on the 9th of April. They will keep on updating. 
* official lock-down dates for each country (various sources).
* dates of first death and 1st reported case. I have also the dates for each additional death and case-confired. 
* world bank measures for GDP per capita, unemployment and other health related indexes. 
* Rule of Law from 2018 (this is a measure that was also included in the antisocial punishment paper by Gachter et al.)
* Number of tests and deaths - though this is sth that needs to be updated ever so often

* from World Value Scales: values such as the importance of democracy and respect for ind. human rights nowadays
* average temperature and humididty for the period between 22nd of January to March 24 (thanks to World Weather Online AP)
* demographics such as median age, population, population density, urban density
* behavioural variables such as the Rule of Law (from World Bank), risk taking propensity, altruism, patience, etc. (from the global preferences survey). 


Some important dates to consider:

* 11/03/2020 : The World Health Organisation declares COVID-19 a pandemic. 
* 21-22/01/2020: First official reports conceding there are human-to-human transmissions.
* 23/02/2020: Start of observation period for mobility restrictions.
* 29/03/2020: end of observation period for 1st Google report. 
* 05/04/2020: end of observation period for 2nd Google report. 
* 22/01/2020 - 24/03/2020: period of weather observation

The idea was to explore the ability of measures such as such the rule of law, importance of democracy and respect of individual rights to correlate with a) policy decisions such as when to apply restriction measures and how severe those are and b) with individual compliance: how much was mobility restricted. 

Disclaimer: this is not an attempt to identify causal relations. This is just a report to satisfy curiocities that were raised over coffee-breaks. If we are to make a more serious scientific claims to motivate this, I guess we can claim the following two-fold contribution:
1. Assessing the usefulness of social-value indexes such as the ROL, etc. 
2. Present a case for future studies, with more ambitious identification strategies, that aim to infer causality. 





# Movement analysis

```{r mobility, fig.cap="Movement restriction across countries"}



df_short%>%
 filter(!is.na(Movement),!is.na(StringencyIndex))%>%
  #mutate(Country=as.factor(Country))%>%
  #group_by(Country)%>%
  mutate(Country = forcats::fct_reorder(Country,-Movement)) %>%

  ggplot()+
  geom_point(aes(x=Country,y=Movement),colour="blue")+
  coord_flip()+
  #geom_vline(xintercept=0,colour="black")+

 # scale_y_discrete()+
  #scale_x_discrete()+
  labs(x = 'Countries',y = "Average Movement")+
  theme_bw()
  #theme(axis.text.x = element_text(angle = 90, hjust = 1))





```

I focus on the dependent variable: "Movement" for the moment. 
This variable is based on Google reports. I aggregate a single score per country by taking the average on all categories (Parks, Groceries...) except "residential". Residential measures: "Movement trends for places of residence". Unlike the other categories, residential goes up during this period (people typically move around their houses more). 
The x-axis displays the average movement reduction across the entire period for every country. 

Movement probably captures two (latent for the moment) variables: a. governments' stricness of enforcing the measures b. citizens' compliance to governments' mandate. 
Although the source is reliable, it's prob. good to keep in mind that there are strong selection issues: the variable captures the behaviour of people who use smartphones and have allowed Google to track their location. 



So, what factors affect change in movement? Here is a regression that avoids variables that are related to Covid statistics (like Stringency Index, number of new cases, etc.).

```{r }




basic<-formula(paste("Movement ~", paste(vars_basic, sep = "", collapse = "+")))
basic_mob<-lm(data=df_short,basic)

briq<-formula(paste("Movement ~", paste(c(vars_basic,vars_briq,"ROL"), sep = "", collapse = "+")))
briq_mob<-lm(data=df_short,briq)

hof<-formula(paste("Movement ~", paste(c(vars_basic,vars_briq,"ROL",vars_hof), sep = "", collapse = "+")))
hof_mob<-lm(data=df_short,hof)

tab_model(basic_mob,briq_mob,hof_mob)





```

Three nested models: basic, with BRIQ/ROL and with hofstede's variables. 
The adjusted R^2 can be improved by discarding variables with high p-values but at this stage I chose to include everything. 
Including other variables drops significantly the number of total observations. 
Many coefficients do not make sense (e.g. risk taking, patience). 
Therefore, either the beh.variables are worthless or, taking the average Movement across the entire period is meaningless. 
Frankly, without accounting for weather, we are not addressing one of the elephants in the room. 


Let's take the average for the period when policies were restrictive. 
That is, let's exclude the days that the stringency index went down. 


```{r }
basic_mob<-lm(data=w1_short,basic)
briq_mob<-lm(data=w1_short,briq)
hof_mob<-lm(data=w1_short,hof)

tab_model(basic_mob,briq_mob,hof_mob)




```


Same story, only with lower adjusted R_squared in the Briq model (middle column) and better hofstede (3rd column). 

# Stringency Index

```{r}
df_short%>%
  filter(!is.na(Movement),!is.na(StringencyIndex))%>%
  #mutate(Country=as.factor(Country))%>%
  #group_by(Country)%>%
  mutate(Country = forcats::fct_reorder(Country,StringencyIndex)) %>%

  ggplot()+
  geom_point(aes(x=Country,y=StringencyIndex),colour="blue")+
  coord_flip()+
  #geom_vline(xintercept=0,colour="black")+

 # scale_y_discrete()+
  #scale_x_discrete()+
  labs(x = 'Countries',y = "Average Stringency Index")+
  theme_bw()
  #theme(axis.text.x = element_text(angle = 90, hjust = 1))

```


What makes a gvt more prone to apply more restrictive measures? 
Once again, I focus on non-covid related parameters. 

Let's consider first, the average SI across the entire period. 

```{r }
basic<-formula(paste("StringencyIndex ~", paste(vars_basic, sep = "", collapse = "+")))
basic_mob<-lm(data=df_short,basic)

briq<-formula(paste("StringencyIndex ~", paste(c(vars_basic,vars_briq,"ROL"), sep = "", collapse = "+")))
briq_mob<-lm(data=df_short,briq)

hof<-formula(paste("StringencyIndex ~", paste(c(vars_basic,vars_briq,"ROL",vars_hof), sep = "", collapse = "+")))
hof_mob<-lm(data=df_short,hof)

tab_model(basic_mob,briq_mob,hof_mob)


```

Higher SI correlates: 

* positively with: GDP_pc, Population density, risk taking, positive reciprocity. 
* negatively with: patience, altruism, prop of age 65+, ROL, Collectivism and uncertainty avoidance.

This analysis makes much more sense.
I particularly like the negative correlation with ROL. 
This means that gvts that anticipate lower adherence, overdo it. 


# Compliance

```{r}
df_short%>%
  filter(!is.na(Movement),!is.na(StringencyIndex))%>%
  #mutate(Country=as.factor(Country))%>%
  #group_by(Country)%>%
  mutate(Country = forcats::fct_reorder(Country,LongRunCoeff_3)) %>%

  ggplot()+
  geom_point(aes(x=Country,y=LongRunCoeff_3),colour="blue")+
  coord_flip()+
  #geom_vline(xintercept=0,colour="black")+

 # scale_y_discrete()+
  #scale_x_discrete()+
  labs(x = 'Countries',y = "Long Run Coef_3")+
  theme_bw()
  #theme(axis.text.x = element_text(angle = 90, hjust = 1))

```

Let's interpret the long run ceof as compliance. 


```{r }
basic<-formula(paste("LongRunCoeff_3 ~", paste(vars_basic, sep = "", collapse = "+")))
basic_mob<-lm(data=df_short,basic)

briq<-formula(paste("LongRunCoeff_3 ~", paste(c(vars_basic,vars_briq,"ROL"), sep = "", collapse = "+")))
briq_mob<-lm(data=df_short,briq)

hof<-formula(paste("LongRunCoeff_3 ~", paste(c(vars_basic,vars_briq,"ROL",vars_hof), sep = "", collapse = "+")))
hof_mob<-lm(data=df_short,hof)

tab_model(basic_mob,briq_mob,hof_mob)


```

When we don't include the Hofstede variables, we find some interesting relations:

* More risk-seeking societies are less compliant
* More trusting and more altruistic ones are more compliant. Perhaps more aged societies are also more compliant. 

Though, the fact that societies that are more reciprocal are also more likely to not comply, spoil the mix. 
Also, what's up with the adjusted R^2 in the third model?!
I haven't looked at the long run coefficient for wave 1, as I need to calculate it again. 
It would be nice to define a function that does this automatically, like I did the coefficients of the simple regressin. 


# Death and cases



```{r trend, fig.cap="Death curve"}

df%>%
  mutate(Date=as.Date(Date))%>%
  mutate(label = if_else(Date == max(Date,na.rm=T) & total_deaths>10000, as.character(Country), NA_character_)) %>%

  #filter(TotalDeaths>1000,Date>as.Date("01/03/2020",format="%d/%m/%y"))%>%
  group_by(Country)%>%
  ggplot(aes(x = Date, y = total_deaths,group=Country, color=Country)) + 
  geom_line(size=1) +
  #scale_colour_brewer(palette = 'Set1')+
  #guides(colour="legend")
 
  scale_color_discrete(guide = FALSE)+
  
  
   theme_minimal()+
  theme(text = element_text(size=15))+
  geom_label_repel(aes(label = label),
                  nudge_x = 1,
                  na.rm = TRUE)


df%>%
  mutate(Date=as.Date(Date))%>%
  mutate(label = if_else(Date == max(Date,na.rm=T) & total_deaths_per_million>200, as.character(Country), NA_character_)) %>%

  #filter(TotalDeaths>1000,Date>as.Date("01/03/2020",format="%d/%m/%y"))%>%
  group_by(Country)%>%
  ggplot(aes(x = Date, y = total_deaths_per_million,group=Country, color=Country)) + 
  geom_line(size=1) +
  #scale_colour_brewer(palette = 'Set1')+
  #guides(colour="legend")
 
  scale_color_discrete(guide = FALSE)+
  
  
   theme_minimal()+
  theme(text = element_text(size=15))+
  geom_label_repel(aes(label = label),
                  nudge_x = 1,
                  na.rm = TRUE)







```

## Non covid correlates

What correlates with death and total cases (that is not related to covid)?

Death:
```{r}
basic<-formula(paste("LogDeath ~", paste(vars_basic, sep = "", collapse = "+")))
basic_mob<-lm(data=df_short,basic)

briq<-formula(paste("LogDeath ~", paste(c(vars_basic,vars_briq,"ROL"), sep = "", collapse = "+")))
briq_mob<-lm(data=df_short,briq)

hof<-formula(paste("LogDeath ~", paste(c(vars_basic,vars_briq,"ROL",vars_hof), sep = "", collapse = "+")))
hof_mob<-lm(data=df_short,hof)

tab_model(basic_mob,briq_mob,hof_mob)

```


* Share of older folks correlates with death from covid 
* wealthier countries have more deaths (not sure if this is intuitive)
* Risk aversion correlates with less death (nice) and higher patience (and altruism) correates with more death (not so nice)
* Countries that obey Rule of Law have lower cases


Cases: 

```{r}
basic<-formula(paste("LogCases ~", paste(vars_basic, sep = "", collapse = "+")))
basic_mob<-lm(data=df_short,basic)

briq<-formula(paste("LogCases ~", paste(c(vars_basic,vars_briq,"ROL"), sep = "", collapse = "+")))
briq_mob<-lm(data=df_short,briq)

hof<-formula(paste("LogCases ~", paste(c(vars_basic,vars_briq,"ROL",vars_hof), sep = "", collapse = "+")))
hof_mob<-lm(data=df_short,hof)

tab_model(basic_mob,briq_mob,hof_mob)

```

* More or less the same as with total deaths. Probably because the two (deaths and cases) are so highly correlated.

Can we quickly justify the role of Movement, as measured by Google and as aggregated by us (average of everything but Residential)?

```{r}

tab_model(lm(data=df_short,LogDeath~Movement))

```


```{r}

tab_model(lm(data=df_short,LogCases~Movement))

```

Sort of.. 
What if we add the most important controls? 


```{r}

briq_NoMove<-formula(paste("LogDeath ~", paste(c(vars_basic,vars_briq,"ROL"), sep = "", collapse = "+")))
briq_mob_NoMove<-lm(data=w1_short,briq_NoMove)

briq<-formula(paste("LogDeath ~", paste(c(vars_basic,vars_briq,"ROL","Movement"), sep = "", collapse = "+")))
briq_mob<-lm(data=w1_short,briq)

briq_SI<-formula(paste("LogDeath ~", paste(c(vars_basic,vars_briq,"ROL","Movement","StringencyIndex"), sep = "", collapse = "+")))
briq_mob_SI<-lm(data=w1_short,briq_SI)


briq_SI<-formula(paste("LogDeath ~", paste(c(vars_basic_GDP,vars_briq,"ROL","Movement","StringencyIndex"), sep = "", collapse = "+")))
briq_mob_SI<-lm(data=df_short,briq_SI)

tab_model(briq_mob_NoMove,briq_mob,briq_mob_SI)

```

Stronger result. We are also able to explain a lot of the variation in the data..! Like 60%... Is this normal?
Even without Movement, R^2 is 50%. It is peculiar given that no variable is drawn from Covid era... 
Moreover, including the Stringency Index in the regression does not improve R2 too much. 

## Death and compliance 


```{r}

tab_model(lm(data=df_short,LogDeath~LongRunCoeff_3))

```


```{r}

briq<-formula(paste("LogDeath ~", paste(c(vars_basic,vars_briq,"ROL","LongRunCoeff_3"), sep = "", collapse = "+")))
briq_mob<-lm(data=df_short,briq)



tab_model(briq_mob)

```

The long run coeff. does not predict much. Though, it is in the right direction.
What about a simple coefficient (no lags)?


```{r}

tab_model(lm(data=df_short,LogDeath~Compliance))

#tab_model(lm(data=df_short,LogDeath~Cor))

```


```{r}

briq<-formula(paste("LogDeath ~", paste(c(vars_basic,vars_briq,"ROL","Compliance"), sep = "", collapse = "+")))
briq_mob<-lm(data=df_short,briq)



tab_model(briq_mob)

```

Better in the simple model but its effect is also crowded out by controls. What if we focus this simpler coefficient to wave 1?
First of all, Movement, affects fundamental indexes (like death) mostly in wave 1. 
Proof:

```{r}
tab_model(lm(data=w1_short,LogDeath~Movement))

tab_model(lm(data=df_short,LogDeath~Movement))

```


```{r}

tab_model(lm(data=w1_short,LogDeath~Compliance))

briq<-formula(paste("LogDeath ~", paste(c(vars_basic,"ROL","Compliance"), sep = "", collapse = "+")))
briq_mob<-lm(data=w1_short,briq)



tab_model(briq_mob)

```

Though again, it is being crowded out eventually. 
We should probably estimate a LongRunCoef for wave 1. 
Hopefully, we get something even stronger. 
If not, then it's worth asking ourselves if it's worth pursuing a fancy model for "compliance". 







